{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadhana3636/Capstone_Project_6_ML_Indigo-Airline_Passenger_Referral_Prediction/blob/main/ML_Capstone_project_IndiGo_Airline_Passenger_Referral_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  IndiGo Airline Passenger Referral Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual - Sadhana B\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Video link :**  https://drive.google.com/file/d/1bMzidcxziCPxMsG7SAWMPkOGdk8LAw4B/view?usp=sharing"
      ],
      "metadata": {
        "id": "Ag9yxD_AQ6i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IndiGo Airline Passenger Referral Prediction** aims to analyze customer reviews from 2006 to 2019 to understand the factors influencing passenger referrals. In the highly competitive airline industry, customer experience plays a pivotal role in building brand loyalty and driving positive word-of-mouth. By leveraging machine learning techniques, this project seeks to develop a predictive model that identifies which passengers are most likely to recommend IndiGo. Insights from this analysis will help the airline refine its services by focusing on key aspects such as in-flight comfort, customer service, and overall value for money.  \n",
        "\n",
        "With a data-driven approach, IndiGo can implement targeted improvements in areas that significantly impact passenger satisfaction. Predicting referrals enables strategic marketing efforts, allowing the airline to capitalize on positive customer feedback and strengthen its brand reputation. Additionally, continuous refinement of service quality based on predictive insights will help IndiGo differentiate itself from competitors and maintain a strong position in the market. By integrating advanced analytics into decision-making, IndiGo can enhance customer engagement, optimize service offerings, and foster long-term brand loyalty."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " GitHub Link :  https://github.com/sadhana3636/Capstone_Project_6_ML_Indigo-Airline_Passenger_Referral_Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IndiGo aims to predict passenger referrals by analyzing customer reviews from 2006 to 2019. The goal is to identify key service factors influencing recommendations and enhance customer satisfaction."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Data Handling & Processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistical analysis\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Preprocessing & Feature Engineering\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Machine Learning Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import time\n",
        "\n",
        "# Model Evaluation\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Define the path to dataset\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/Alma_FoundationTrack/AlmaBetter_Capstone_Project_6/data_airline_reviews.xlsx - capstone_airline_reviews3.csv\"\n",
        "\n",
        "reviews_df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "L1a7mFAzULMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "reviews_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "\n",
        "# Count the number of rows and columns\n",
        "num_rows, num_columns = reviews_df.shape\n",
        "\n",
        "print(f\"Number of Rows: {num_rows}\")\n",
        "print(f\"Number of Columns: {num_columns}\")\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "\n",
        "reviews_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "duplicate_count = reviews_df.duplicated().sum()\n",
        "print(f\"Number of Duplicate Rows: {duplicate_count}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the duplicate rows\n",
        "\n",
        "reviews_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "ATA9Xrils49b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df.shape"
      ],
      "metadata": {
        "id": "TNbrRTK8cl6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "\n",
        "missing_values = reviews_df.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of missing values\n",
        "\n",
        "\n",
        "missing_percentage = (missing_values / len(reviews_df)) * 100\n",
        "print(\"Percentage of Missing Values:\\n\", round(missing_percentage,2))"
      ],
      "metadata": {
        "id": "hhlLGENiWHxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar graph representation of missing values\n",
        "\n",
        "\n",
        "# Calculate missing percentages\n",
        "missing_percent = reviews_df.isnull().mean() * 100\n",
        "\n",
        "# Plot missing values as bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = missing_percent.sort_values(ascending=False).plot(kind='bar', color='orange')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, v in enumerate(missing_percent.sort_values(ascending=False)):\n",
        "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "plt.title('Percentage of Missing Values by Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Percentage Missing')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ZEgOlfEQtxhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The Airline Reviews dataset has 1,31,895 rows and 17 columns.\n",
        "2. Number of duplicate rows is 70711 and these duplicate rows were dropped.\n",
        "3. It is found that 69% of data in 'aircraft' column is missing.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "reviews_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "reviews_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 17 columns present in the dataset.\n",
        "1. **airline :** Name of the airline.\n",
        "2. **overall :** Overall points are given to the trip between 1 and 10.\n",
        "3. **author :** Author of the trip\n",
        "4. **review_date :** Date of the Riview\n",
        "5. **customer_review :**Review from the customer on travel experience.\n",
        "6. **aircraft :** Type of the aircraft\n",
        "7. **traveller_type :** Type of the traveler(e.g. business,leisure)\n",
        "8. **cabin :** Type of cabin\n",
        "9. **route :** Route of flight( from which place to which place)\n",
        "10. **date_flown :** Date of fly\n",
        "11. **seat_comfort :** Rated between 1-5\n",
        "12. **cabin_service :** Rated between 1-5\n",
        "13. **food_bev :** Rated between 1-5\n",
        "14. **entertainment :** Rated between 1-5\n",
        "15. **ground_service :** Rated between 1-5\n",
        "16. **value_for_money :** Rated between 1-5\n",
        "17. **recommended :** target variable (yes or no)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of unique values in each column\n",
        "\n",
        "unique_values = reviews_df.nunique()\n",
        "print(\"Number of Unique Values in Each Column:\\n\", unique_values)\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique values of the column 'trveller_type'\n",
        "\n",
        "unique_values = reviews_df['traveller_type'].unique()\n",
        "print(\"Unique Values in 'traveller_type' Column:\\n\", unique_values)"
      ],
      "metadata": {
        "id": "lCKpJI48xeFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique values of the column 'cabin'\n",
        "\n",
        "unique_values = reviews_df['cabin'].unique()\n",
        "print(\"Unique Values in 'cabin' Column:\\n\", unique_values)"
      ],
      "metadata": {
        "id": "U8hVlHQTxpBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique values of the column 'recommended'\n",
        "\n",
        "unique_values = reviews_df['recommended'].unique()\n",
        "print(\"Unique Values in 'recommended' Column:\\n\", unique_values)"
      ],
      "metadata": {
        "id": "2lOkTA30xwJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df.head()"
      ],
      "metadata": {
        "id": "C53QXQiryIAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# We have dropped duplicate rows."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. We have dropped duplicate rows.\n",
        "\n",
        "2. Found the total number of missing values in each column and visualized it using graph."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "i8wXD2R-GZkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 : Top 10 Airlines with Largest Flights Conducted - Bar Chart"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code- Bar chart\n",
        "\n",
        "\n",
        "# Count the number of occurrences (flights) for each airline\n",
        "top_airlines = reviews_df['airline'].value_counts().nlargest(10)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = top_airlines.plot(kind='bar', color='green')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for i, v in enumerate(top_airlines):\n",
        "    plt.text(i, v + 10, str(v), ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "plt.title('Top 10 Airlines with Largest Flights Conducted')\n",
        "plt.xlabel('Airlines')\n",
        "plt.ylabel('Number of Flights')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a **bar chart** because it effectively visualizes the **count of flights** for each airline, making it easy to compare the **top 10 airlines** and display exact values with labels."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the **top 10 airlines with the largest number of flights**, highlighting the market leaders. It shows which airlines dominate in terms of **flight volume**, indicating their operational scale.\n",
        "\n",
        "\n",
        "*   Here we find that the highest number of trips conducted by the airline **'Spirit Airlines'** with a count of **2871**.\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 : Top 10 Most Frequently Used Aircrafts - Bar chart"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code - bar chart\n",
        "\n",
        "# Count the occurrences of each aircraft type\n",
        "aircraft_counts = reviews_df['aircraft'].value_counts().head(10)  # Top 10 aircrafts\n",
        "\n",
        "# Plot the chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "ax = aircraft_counts.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Top 10 Most Frequently Used Aircrafts')\n",
        "plt.xlabel('Aircraft')\n",
        "plt.ylabel('Number of Flights')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display values on top of bars\n",
        "for i in ax.containers:\n",
        "    ax.bar_label(i, fmt='%d', label_type='edge', fontsize=10)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a **bar chart** because it effectively displays the **frequency of aircraft usage** with clear, distinct bars, making it easy to compare the top 10 models. It also allows for adding value labels for better readability."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that **A320** is the most frequently used aircraft for flights, indicating its popularity and reliability. The **top 10 aircraft** account for a significant portion of the total flights conducted."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 : Top 10 Most Chosen Routes by Customers - Horizontal Bar chart"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization - Horizontal Bar chart\n",
        "\n",
        "# Top 10 most chosen routes\n",
        "top_routes = reviews_df['route'].value_counts().head(10)\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(12, 6))\n",
        "ax = sns.barplot(x=top_routes.values, y=top_routes.index, palette='coolwarm')\n",
        "# Display count labels near the bars\n",
        "for i, value in enumerate(top_routes.values):\n",
        "    ax.text(value + 5, i, f'{value}', color='black', ha='left', va='center', fontsize=10)\n",
        "plt.title('Top 10 Most Chosen Routes by Customers')\n",
        "plt.xlabel('Number of Flights')\n",
        "plt.ylabel('Route')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **horizontal bar chart** is chosen because it effectively displays the **top 10 most chosen routes** with clear labels and space for count values, making it easy to compare the popularity of different routes."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the **most frequently chosen routes** by customers, indicating popular travel corridors. This insight helps airlines **prioritize services and resources** on high-demand routes.\n",
        "Most preferred route is from BKK to LHR."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 : Customer Recommendations (Yes/No) -  Pie chart"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code - Pie chart\n",
        "\n",
        "# Count the number of recommendations\n",
        "recommendation_counts = reviews_df['recommended'].value_counts()\n",
        "\n",
        "# Plotting the pie chart\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(recommendation_counts, labels=recommendation_counts.index, autopct='%1.1f%%', startangle=140, colors=['#4caf50', '#f44336'])\n",
        "plt.title('Customer Recommendations (Yes/No)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **pie chart** is chosen because it effectively shows the **proportion of customer recommendations** in an easy-to-interpret visual format. It clearly highlights the **distribution between positive and negative feedback**."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that a **majority of customers do not recommend** the airline, indicating **dissatisfaction with the service**. The smaller proportion of positive recommendations suggests **room for improvement**."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "CCGDItXcqCE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 : Relationship between Traveller Type and Recommendation - Stacked bar chart"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code - stacked bar chart\n",
        "\n",
        "\n",
        "# Create a stacked bar chart\n",
        "fig, ax = plt.subplots(figsize=(12, 5))\n",
        "stacked_bar = pd.crosstab(reviews_df['traveller_type'], reviews_df['recommended']).plot(\n",
        "    kind='bar', stacked=True, colormap='viridis', ax=ax\n",
        ")\n",
        "# Add count labels on the bars\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, label_type='center', fmt='%d', color='white', fontsize=10)\n",
        "# Labels and title\n",
        "plt.title('Relationship between Traveller Type and Recommendation')\n",
        "plt.xlabel('Traveller Type')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Recommended', labels=['No', 'Yes'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It clearly shows the distribution of recommendations (Yes/No) within each traveller type, making it easy to compare how likely different traveller groups (e.g., solo, family, business) are to recommend the airline."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It highlights potential patterns, such as whether business travellers are more likely to recommend the airline compared to leisure travellers.\n",
        "\n",
        "It is clear that people who travel with the reasons 'Business', 'Couple Leisure', 'Family Leisure' recommend more compared to 'Solo Leisure'."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights gained from the stacked bar chart can lead to a positive business impact.\n",
        "\n",
        "*   The chart reveals that business travelers and family leisure travelers are more likely to recommend the airline, indicating higher satisfaction levels.\n",
        "*   Targeted marketing strategies can be created to retain and attract more of these customer segments by offering tailored benefits, such as loyalty programs, family-friendly packages, or business-class discounts.\n",
        "*    Improving the travel experience for solo leisure travelers, who are less likely to recommend the airline, can also enhance overall customer satisfaction."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 : Relationship Between Cabin and Recommended - Grouped bar chart"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code - Grouped bar chart\n",
        "\n",
        "\n",
        "# Count the occurrences of each combination of 'cabin' and 'recommended'\n",
        "cabin_recommend_counts = reviews_df.groupby(['cabin', 'recommended']).size().unstack()\n",
        "# Plotting the grouped bar chart\n",
        "plt.figure(figsize=(12, 5))\n",
        "cabin_recommend_counts.plot(kind='bar', stacked=False, colormap='cividis', ax=plt.gca())\n",
        "# Add value labels on the bars\n",
        "for bars in plt.gca().containers:\n",
        "    plt.gca().bar_label(bars, fmt='%d', label_type='edge', fontsize=10)\n",
        "plt.title('Relationship Between Cabin and Recommended')\n",
        "plt.xlabel('Cabin Type')\n",
        "plt.ylabel('Count')\n",
        "plt.legend(title='Recommended', labels=['Not Recommended', 'Recommended'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grouped bar chart is chosen because it effectively compares the relationship between **cabin classes** and **recommendations**, making it easy to visualize how preferences vary across different cabins. It clearly displays both the distribution and the recommendation patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals that **Business and Premium Economy passengers** are more likely to recommend the airline compared to **Economy passengers**, indicating higher satisfaction in premium cabins. **Economy class** shows a lower recommendation rate, suggesting potential service improvement areas."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights highlight that premium cabin passengers are more satisfied and likely to recommend the airline. This presents an opportunity to expand premium services and offer loyalty programs, attracting more high-value customers.\n",
        "\n",
        "**Potential Negative Growth:**\n",
        "\n",
        "The lower recommendation rate in Economy class suggests potential dissatisfaction, which could lead to negative reviews and reduced customer retention. Addressing service quality in Economy class is essential to prevent losing price-sensitive customers."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Ananlysis"
      ],
      "metadata": {
        "id": "AsFpDB09ftkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 : Overall Service Rating by Cabin Class and Traveller Type - Grouped Bar chart"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code - Grouped Bar chart\n",
        "\n",
        "# Create a grouped DataFrame for average service rating by cabin and traveller type\n",
        "service_by_cabin_traveller = reviews_df.groupby(['cabin', 'traveller_type'])['cabin_service'].mean().unstack()\n",
        "# Plotting Grouped Bar Chart\n",
        "plt.figure(figsize=(14, 4))\n",
        "service_by_cabin_traveller.plot(kind='bar', colormap='viridis', ax=plt.gca())\n",
        "plt.title('Overall Service Rating by Cabin Class and Traveller Type (Grouped)')\n",
        "plt.xlabel('Cabin Class')\n",
        "plt.ylabel('Average Service Rating')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(title='Traveller Type')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The grouped bar chart is chosen because it allows for a clear side-by-side comparison of service ratings across different 'cabin' classes and 'traveller_type' categories, making it easy to identify patterns and differences."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall service rating in economy class is very poor."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Positive Business Impact:\n",
        "The insight regarding higher service ratings in Business and First Class highlights the effectiveness of premium services. IndiGo can leverage this by promoting loyalty programs and offering exclusive deals to frequent Business travellers, boosting customer retention and profitability.\n",
        "\n",
        "\n",
        "* Potential Negative Growth:\n",
        "The poor service ratings in Economy Class could deter repeat customers and harm the airline's reputation. To prevent negative growth, IndiGo should improve economy-class service by enhancing comfort, addressing customer complaints, and providing better value-for-money experiences."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 : Overall Service Rating Over Time - Line Chart"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code - Line Chart\n",
        "\n",
        "#  Convert 'review_date' to datetime format\n",
        "reviews_df['review_date'] = pd.to_datetime(reviews_df['review_date'], errors='coerce')\n",
        "\n",
        "#  Extract Year-Month for grouping\n",
        "reviews_df['year_month'] = reviews_df['review_date'].dt.to_period('M')\n",
        "\n",
        "#  Group by Year-Month and Calculate Average Overall Rating\n",
        "rating_over_time = reviews_df.groupby('year_month')['overall'].mean()\n",
        "\n",
        "#  Plotting the Trend Over Time\n",
        "plt.figure(figsize=(14, 5))\n",
        "rating_over_time.plot(color='royalblue', marker='o', linestyle='-', linewidth=2, markersize=6)\n",
        "\n",
        "plt.title('Overall Service Rating Over Time')\n",
        "plt.xlabel('Time (Monthly)')\n",
        "plt.ylabel('Average Overall Rating')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The **line chart** is chosen because it effectively visualizes **trends over time**, making it easy to identify **patterns, fluctuations, and long-term changes** in the overall service rating. It clearly shows whether the rating is improving, declining, or remaining stable over different periods."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The **overall service rating** shows **fluctuations over time**, indicating **variations in customer satisfaction** during different periods. A **consistent decline or sharp drops** in specific months or years may highlight **service issues** or negative incidents, while upward trends suggest **improvements or positive experiences**.\n",
        "\n",
        "There is drastice decline in overall rating around the year 2008 and 2009.\n",
        "Within a year overall rating found a quick spike upwards nearly in 2010.\n",
        "We can see a gradual decrease in overall rating by the costomers rom the year 2010 to 2019.\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  **Will the gained insights help create a positive business impact?**\n",
        "Yes, the insights can positively impact the business by identifying periods of poor customer satisfaction. IndiGo can analyze the root causes behind the 2008-2009 decline (e.g., service issues, operational challenges) and ensure similar mistakes are avoided. Additionally, they can replicate the strategies that led to the 2010 improvement to enhance customer satisfaction.\n",
        "\n",
        "* **Are there any insights that lead to negative growth? Justify with specific reason.**\n",
        "Yes, the gradual decline in overall rating from 2010 to 2019 indicates a deteriorating customer experience. If this trend continues, it could damage the airline’s reputation, reduce customer loyalty, and ultimately impact revenue growth. IndiGo needs to address customer concerns promptly to prevent further decline."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 : Value for Money by Cabin Type - Violin Plot"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code - Violin plot\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "sns.violinplot(x='cabin', y='value_for_money', data=reviews_df, palette='muted')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Value for Money by Cabin Type (Violin Plot)')\n",
        "plt.xlabel('Cabin Type')\n",
        "plt.ylabel('Value for Money')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin plot is used to visualize the distribution and density of value_for_money across different cabin types. It combines a box plot with a kernel density plot, making it easy to see both the distribution shape and variability in the data."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Wider Distribution Indicates Variability: The broader shape of the violin plot in Business and First-class cabins shows greater variability in perceived value, meaning some passengers find it extremely valuable while others may not.\n",
        "\n",
        "* Narrower Distribution Shows Consistency: The narrower shape in Economy indicates that most passengers rate the value similarly, reflecting a consistent but lower satisfaction level.\n",
        "\n",
        "* Median Line Shows Central Tendency: The median lines reveal the typical value-for-money rating for each cabin class, helping to identify which classes are generally rated higher or lower.\n",
        "* Clearly Business class and First class passengers are more satisfied with the facility.\n",
        "* Bottom is wide in Economy classs and premium economy class which says that those passengers wre not happy with the travel facilities.\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Chart 10 :  Overall Rating vs Recommended- stacked bar chart"
      ],
      "metadata": {
        "id": "UPu0J-KYwBIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for visualization -stacked bar chart\n",
        "\n",
        "# Create a grouped bar plot\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.barplot(x='overall', y='recommended', data=reviews_df, ci=None, estimator=lambda x: sum(x)/len(x), palette='coolwarm')\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Overall Rating vs Recommended')\n",
        "plt.xlabel('Overall Rating')\n",
        "plt.ylabel('Percentage Recommended')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Display the chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Sd7zJGx8wAn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "Ps7CvO9twYSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The grouped bar chart effectively shows the relationship between overall ratings and the percentage of recommendations, making it easy to compare how customer satisfaction influences recommendations. The color contrast helps in quickly identifying patterns and trends."
      ],
      "metadata": {
        "id": "SIt3FAbGwZ-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "5QuKO9Y9whZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passengers who gave higher overall ratings are significantly more likely to recommend the airline. Conversely, lower overall ratings correspond to fewer recommendations, indicating that customer satisfaction strongly impacts referral likelihood."
      ],
      "metadata": {
        "id": "FTbLJVIXwl7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?\n",
        "\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "YlozViNWww6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights will help create a positive business impact. By identifying the strong correlation between overall rating and recommendations, the airline can focus on improving service quality to enhance customer satisfaction, ultimately driving more referrals and customer retention."
      ],
      "metadata": {
        "id": "rZkkdgcnwyVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 :  Distribution of variables - Histogram"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code - Histogram\n",
        "\n",
        "# Plot histograms for numerical variables\n",
        "numerical_cols = reviews_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(14, 10))\n",
        "# Loop through each numerical column and create subplots\n",
        "for i, col in enumerate(numerical_cols, 1):\n",
        "    plt.subplot(4, 4, i)  # Adjust grid size based on number of columns\n",
        "    plt.hist(reviews_df[col].dropna(), bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'Distribution of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram was chosen because it effectively visualizes the distribution of numerical variables, helping to identify patterns, such as data skewness, outliers, and central tendencies. It provides a clear overview of how frequently different values occur in each variable."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among all cabin service is found to have better ratings.\n",
        "\n",
        "Overall rating is not satisfactory.\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Will the gained insights help create a positive business impact?\n",
        "\n",
        "Yes, the gained insights can drive a positive business impact.\n",
        "\n",
        "Since cabin service is rated higher, the airline can leverage this strength in their marketing campaigns by promoting their superior cabin service, attracting more customers.\n",
        "The focus on maintaining and enhancing cabin service can help in boosting customer satisfaction and loyalty, ultimately leading to better reviews and recommendations.\n",
        "\n",
        "* Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "Yes, the overall rating being unsatisfactory indicates a potential risk of negative growth.\n",
        "\n",
        "Low overall ratings may reflect poor customer experiences in certain areas, discouraging new or returning passengers.\n",
        "This could lead to reduced bookings and a decline in the airline's reputation if the underlying service issues (other than cabin service) are not addressed."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 : Heatmap"
      ],
      "metadata": {
        "id": "JJRPfblMxcz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart Visualization code-  Heatmap\n",
        "\n",
        "# Select only numerical columns for the heatmap\n",
        "numerical_cols = reviews_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(numerical_cols.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, square=True, cbar=True)\n",
        "\n",
        "# Add title\n",
        "plt.title('Correlation Heatmap of Numerical Columns', fontsize=16)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A-zxEVpqxigh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "RkRcV30Vju7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heatmap is chosen because it effectively visualizes the correlation between numerical variables, helping to identify strong or weak relationships. It provides a clear overview of feature dependencies, aiding in feature selection and multicollinearity detection."
      ],
      "metadata": {
        "id": "Hy8KRIlJjwY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "4PMiJ2TMkDOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap reveals relationships and patterns between numerical variables, highlighting which features are strongly or weakly correlated. This helps identify key influencers and potential multicollinearity issues in the dataset.\n",
        "Here clearly we can a positive correlation between overall and value_for_money columns with a value 0.89.\n",
        "Also there exists a negative correlation of0.60 between entertainment and the column cabin_service."
      ],
      "metadata": {
        "id": "F9Y-PWUTkJrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "OrZ6olMBkbSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The **positive correlation** between 'overall' and 'value_for_money' (0.89) suggests that **improving the value-for-money experience** directly boosts overall satisfaction, enabling the airline to **enhance customer retention** and loyalty.\n",
        "\n",
        "* The **negative correlation** between 'entertainment' and 'cabin_service' (-0.60) indicates that **poor entertainment quality** might be lowering cabin service ratings, potentially leading to **negative customer experiences** and dissatisfaction."
      ],
      "metadata": {
        "id": "AVSmPwYvkd7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three hypothetical statements can be stated as follows:\n",
        "\n",
        "* **Hypothesis 1: Service Quality and Recommendations**\n",
        "\n",
        "Null Hypothesis (H₀): There is no significant relationship between the airline's cabin service rating and whether a customer recommends the airline.\n",
        "Alternative Hypothesis (H₁): There is a significant relationship between the cabin service rating and customer recommendations.\n",
        "\n",
        "* **Hypothesis 2: Traveller Type and Value for Money**\n",
        "\n",
        "Null Hypothesis (H₀): Traveller type (Business, Leisure, etc.) does not significantly affect the perceived value for money rating.\n",
        "Alternative Hypothesis (H₁): Traveller type has a significant impact on the value for money rating.\n",
        "\n",
        "* **Hypothesis 3: Overall Rating and Recommendation Likelihood**\n",
        "\n",
        "Null Hypothesis (H₀): There is no significant difference in the overall rating between customers who recommend and those who don't.\n",
        "Alternative Hypothesis (H₁): Customers who recommend the airline give a higher overall rating than those who do not."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1: Service Quality and Recommendations**\n",
        "\n",
        "**Null Hypothesis (H₀):** There is no significant relationship between the airline's cabin service rating and whether a customer recommends the airline.\n",
        "\n",
        " **Alternative Hypothesis (H₁):** There is a significant relationship between the cabin service rating and customer recommendations."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "\n",
        "# Create a contingency table between 'cabin_service' and 'recommended'\n",
        "contingency_table = pd.crosstab(reviews_df['cabin_service'], reviews_df['recommended'])\n",
        "\n",
        "# Perform Chi-Square Test\n",
        "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
        "\n",
        "# Display the results\n",
        "print(\"Chi-Square Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"\\n Reject the Null Hypothesis: There is a significant relationship between cabin service rating and customer recommendations.\")\n",
        "else:\n",
        "    print(\"\\n Fail to Reject the Null Hypothesis: There is no significant relationship between cabin service rating and customer recommendations.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test is used here. The P-Value is shown as 0.0 . Because P-Value is very near to 0."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Categorical Variables: Both cabin_service and recommended are categorical, making the Chi-Square test appropriate.\n",
        "\n",
        "* Test for Association: It evaluates whether there is a significant relationship between the two variables.\n",
        "* Contingency Table Analysis: Compares observed and expected frequencies to detect dependencies.\n",
        "* Non-Parametric Test: Suitable for categorical data without assuming normal distribution.\n",
        "* Measures Independence: Tests if customer recommendations depend on cabin service ratings."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypothesis 2: Traveller Type and Value for Money\n",
        "\n",
        "**Null Hypothesis (H₀):** Traveller type (Business, Leisure, etc.) does not significantly affect the perceived value for money rating.\n",
        "\n",
        "**Alternative Hypothesis (H₁):** Traveller type has a significant impact on the value for money rating."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value-\n",
        "# Chi-Square Test of Independence\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(reviews_df['traveller_type'], reviews_df['value_for_money'])\n",
        "\n",
        "# Perform the Chi-Square test\n",
        "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Display results\n",
        "print(\"Chi-Square Statistic:\", chi2)\n",
        "print(\"Degrees of Freedom:\", dof)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Significant relationship between traveller type and value for money.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant relationship between traveller type and value for money.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test of Independence."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chi-Square Test of Independence is used here, because\n",
        "* Both traveller_type and value_for_money are categorical variables.\n",
        "* The Chi-Square test evaluates whether there is a statistically significant association between the two categorical variables.\n",
        "* Suitable for analyzing frequency distribution across categories."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 3: Overall Rating and Recommendation Likelihood**\n",
        "\n",
        "**Null Hypothesis (H₀):** There is no significant difference in the overall rating between customers who recommend and those who don't.\n",
        "**Alternative Hypothesis (H₁):** Customers who recommend the airline give a higher overall rating than those who do not."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value : Independent Samples T-Test\n",
        "\n",
        "\n",
        "# Splitting data into two groups: Recommended and Not Recommended\n",
        "group_recommended = reviews_df[reviews_df['recommended'] == 'yes']['overall'].dropna()\n",
        "group_not_recommended = reviews_df[reviews_df['recommended'] == 'no']['overall'].dropna()\n",
        "\n",
        "# Perform Independent Samples T-Test\n",
        "t_stat, p_value = stats.ttest_ind(group_recommended, group_not_recommended, equal_var=False)\n",
        "\n",
        "# Display results\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Significant difference in overall rating between recommended and not recommended customers.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference in overall rating between the two groups.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Samples T-Test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Samples T-Test. Bacause,\n",
        "* overall is a continuous numerical variable (rating scale).\n",
        "* recommended is a categorical binary variable (Yes/No or 1/0).\n",
        "* The T-test compares the means of the overall rating between the two groups (recommended vs. not recommended).\n",
        "* It tests if the difference in means is statistically significant.\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using another test"
      ],
      "metadata": {
        "id": "ruKGb1jPvH1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test whether data is normally distributed or not : Kolmogorov-Smirnov Test\n",
        "* p > 0.05 → Normally distributed → Use T-Test.\n",
        "\n",
        "* p ≤ 0.05 → Not normally distributed → Use Mann-Whitney U Test."
      ],
      "metadata": {
        "id": "cfe6TikrvQbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kolmogorov-Smirnov Test-\n",
        "\n",
        "from scipy.stats import kstest\n",
        "\n",
        "# Perform Kolmogorov-Smirnov test\n",
        "stat, p_value = kstest(reviews_df['overall'].dropna(), 'norm', args=(reviews_df['overall'].mean(), reviews_df['overall'].std()))\n",
        "\n",
        "print(f\"Kolmogorov-Smirnov Statistic: {stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value > 0.05:\n",
        "    print(\"Fail to Reject Null Hypothesis: Data is normally distributed.\")\n",
        "else:\n",
        "    print(\"Reject Null Hypothesis: Data is NOT normally distributed.\")\n"
      ],
      "metadata": {
        "id": "fxtZUjK-vIIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mann-Whitney U Test: If the overall rating distribution is not normally distributed.\n"
      ],
      "metadata": {
        "id": "gh-cqDnHtl5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Mann-Whitney U Test\n",
        "u_stat, p_value_u = stats.mannwhitneyu(group_recommended, group_not_recommended)\n",
        "# Display results\n",
        "print(\"Mann-Whitney U Statistic:\", u_stat)\n",
        "print(\"P-Value:\", p_value_u)\n",
        "# Interpretation\n",
        "if p_value_u < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Significant difference between recommended and not recommended customers.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference between the two groups.\")\n"
      ],
      "metadata": {
        "id": "6P7lhPhFtlE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data of 'overall' columns is not normally distributed we conducted hypothesis test :  Mann-Whitney U Test."
      ],
      "metadata": {
        "id": "2z9T-fKimTy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "missing_values = reviews_df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of missing values\n",
        "\n",
        "missing_percentage = (missing_values / len(reviews_df)) * 100\n",
        "print(\"Percentage of Missing Values:\\n\", round(missing_percentage,2))"
      ],
      "metadata": {
        "id": "De5xzI2ioJdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly 69 % values of column 'aircraft' is missing. so we can drop that column."
      ],
      "metadata": {
        "id": "LOQA8q7YoL5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy of dataframe is created for preprocessing.\n",
        "\n",
        "reviews_pre_df = reviews_df.copy()\n"
      ],
      "metadata": {
        "id": "_JvkdoJJoobx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code to drop column 'aircraft'\n",
        "reviews_pre_df.drop('aircraft', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "qnNv9gixohPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Column 'author' is not much contributing to the required data.\n",
        "2. 'review_date' is not much importance as it will be same as fly date.\n",
        "Hence we can drop these two columns."
      ],
      "metadata": {
        "id": "0rvKd1cDpXE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the columns 'author' and 'review_date'\n",
        "reviews_pre_df.drop(['author', 'review_date'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ezcD5EyCpVZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = reviews_pre_df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "diqG3q2lqXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Drop rows with minor missing values\n",
        "reviews_pre_df.dropna(subset=['airline', 'customer_review'], inplace=True)"
      ],
      "metadata": {
        "id": "y_6e0EyCqW7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select numerical columns\n",
        "num_cols = reviews_pre_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "\n",
        "# Display numerical columns\n",
        "print(\"Numerical Columns:\", num_cols)\n"
      ],
      "metadata": {
        "id": "NvX2pYqFipzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Median Imputation for Numerical Columns\n",
        "for col in num_cols:\n",
        "    reviews_pre_df[col].fillna(reviews_df[col].median(), inplace=True)"
      ],
      "metadata": {
        "id": "tpmAqyeetmLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mode Imputation for Categorical Columns\n",
        "cat_cols = ['traveller_type', 'cabin', 'route', 'recommended', 'date_flown', 'year_month']\n",
        "for col in cat_cols:\n",
        "    reviews_pre_df[col].fillna(reviews_df[col].mode()[0], inplace=True)"
      ],
      "metadata": {
        "id": "83YqOCZ1tqg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if missing values are handled\n",
        "print(\"Missing values after imputation:\")\n",
        "print(reviews_pre_df.isnull().sum())"
      ],
      "metadata": {
        "id": "QIuZSIFAtv_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dropped Irrelevant Columns : Columns dropped 'author', 'aircraft','review_date' as they had too many missing values.\n",
        "2. Dropped Rows with Rare Missingness: Only rows with missing values were dropped from the columns 'airline', 'customer_review'.\n",
        "3.  Median Imputation is used for Numerical Columns.\n",
        "4. Mode imputaion is used for Categorical columns.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "\n",
        "# Plotting box plots for numerical columns\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(x=reviews_df[col], color='skyblue')\n",
        "    plt.title(f'Box Plot of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting distribution for numerical columns- histogram\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, col in enumerate(num_cols, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.histplot(reviews_df[col], kde=True, color='orange', bins=30)\n",
        "    plt.title(f'Distribution of {col}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6en8HO50xoHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is visualized for outliers using boxplot and histograms. There are no visible outliers found.\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "hGaSGijfeNJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Identify categorical columns\n",
        "categorical_cols = reviews_pre_df.select_dtypes(include=['object', 'category']).columns\n",
        "print(\"Categorical Columns:\", categorical_cols)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding binary values using Label Encoding\n",
        "# Apply Label Encoding to binary columns\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# 'recommended' column has binary values\n",
        "reviews_pre_df['recommended_encoded'] = label_encoder.fit_transform(reviews_pre_df['recommended'])\n",
        "\n",
        "# Encode all binary columns\n",
        "binary_cols = ['recommended']  # Add more binary columns if any\n",
        "for col in binary_cols:\n",
        "    reviews_pre_df[col + '_encoded'] = label_encoder.fit_transform(reviews_pre_df[col])\n",
        "\n",
        "# Check encoding\n",
        "reviews_pre_df[['recommended', 'recommended_encoded']].head()\n"
      ],
      "metadata": {
        "id": "ehXIRflle8A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For columns with multiple categories (e.g., traveller_type, cabin), using One-Hot Encoding.\n",
        "\n",
        "# One-Hot Encoding for multi-class categorical columns\n",
        "multi_class_cols = ['traveller_type', 'cabin']\n",
        "\n",
        "# Perform One-Hot Encoding\n",
        "reviews_pre_df = pd.get_dummies(reviews_pre_df, columns=multi_class_cols, drop_first=True)\n",
        "\n",
        "# Check the new columns\n",
        "print(reviews_pre_df.columns)"
      ],
      "metadata": {
        "id": "qY4CW5RTfKlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "Ojk7q9s3gIl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency encoding(For High Cardinality Columns)- with many unique categories.\n",
        "\n",
        "# Frequency encoding\n",
        "freq_encoding = reviews_pre_df['route'].value_counts(normalize=True)\n",
        "reviews_pre_df['route_encoded'] = reviews_pre_df['route'].map(freq_encoding)\n",
        "\n",
        "# Check encoding\n",
        "reviews_pre_df[['route', 'route_encoded']].head()"
      ],
      "metadata": {
        "id": "qTfPEkiXg8TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify the encoded columns\n",
        "\n",
        "# Check the encoded dataset\n",
        "print(reviews_pre_df.head())\n",
        "reviews_pre_df.shape"
      ],
      "metadata": {
        "id": "Z26BZ8hhhfaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.info()"
      ],
      "metadata": {
        "id": "6gdGGCRWhwRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Encoding\n",
        "airline_freq = reviews_pre_df['airline'].value_counts(normalize=True)  # Frequency of each airline\n",
        "reviews_pre_df['airline_freq_encoded'] = reviews_pre_df['airline'].map(airline_freq)\n",
        "\n",
        "# Drop original column to avoid redundancy\n",
        "reviews_pre_df.drop('airline', axis=1, inplace=True)\n",
        "\n",
        "print(\" Airline column encoded successfully!\")\n",
        "reviews_pre_df[['airline_freq_encoded']].head()\n"
      ],
      "metadata": {
        "id": "tQ6DSYFPiuyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping customer_review column- as it is not directly useful here.\n",
        "reviews_pre_df.drop('customer_review', axis=1, inplace=True)\n",
        "print(\" Customer review column dropped\")\n",
        "\n",
        "# Dropping original route column- as it is already encoded\n",
        "reviews_pre_df.drop('route', axis=1, inplace=True)\n",
        "print(\" Route column dropped (already encoded)\")"
      ],
      "metadata": {
        "id": "0vfO3JrRjGuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the 'recommended' column\n",
        "reviews_pre_df.drop('recommended', axis=1, inplace=True)\n",
        "print(\" 'recommended' column dropped successfully\")"
      ],
      "metadata": {
        "id": "4xXKEfAmj8ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure date_flown is in datetime format\n",
        "reviews_pre_df['date_flown'] = pd.to_datetime(reviews_pre_df['date_flown'], errors='coerce')\n",
        "\n",
        "# Convert date to ordinal format\n",
        "reviews_pre_df['date_flown_ordinal'] = reviews_pre_df['date_flown'].map(lambda x: x.toordinal() if pd.notnull(x) else np.nan)\n",
        "\n",
        "# Drop the original 'date_flown' column\n",
        "reviews_pre_df.drop('date_flown', axis=1, inplace=True)\n",
        "\n",
        "print(\" date_flown converted to ordinal format and original column dropped.\")"
      ],
      "metadata": {
        "id": "xmojKsC0y94O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting datatype of 'year_month' to float type"
      ],
      "metadata": {
        "id": "1y-9T4cYzdg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert period to float format (YYYY.MM)\n",
        "reviews_pre_df['year_month_float'] = reviews_pre_df['year_month'].dt.year + (reviews_pre_df['year_month'].dt.month / 12)\n",
        "\n",
        "# Drop the original period column\n",
        "reviews_pre_df.drop('year_month', axis=1, inplace=True)\n",
        "\n",
        "print(\" year_month converted to float format (YYYY.MM) and original column dropped.\")\n",
        "print(reviews_pre_df[['year_month_float']].head())\n"
      ],
      "metadata": {
        "id": "TkxFXkj_zbmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display final dataset info\n",
        "print(reviews_pre_df.info())\n"
      ],
      "metadata": {
        "id": "KAm8dF-Ejh3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for missing values\n",
        "print(reviews_pre_df.isnull().sum())"
      ],
      "metadata": {
        "id": "-jNqdmiPzu1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "G37JRWelmPOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'date_flown_ordinal' column\n",
        "reviews_pre_df.drop('date_flown_ordinal', axis=1, inplace=True)\n",
        "\n",
        "print(\" Column 'date_flown_ordinal' has been dropped successfully.\")\n"
      ],
      "metadata": {
        "id": "4TP8IUia1b05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.shape"
      ],
      "metadata": {
        "id": "byLOyT-MmXCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Encoded the binary values of the column 'recommended' using Label Encoding.\n",
        "2. Columns with multiple categories are encoded using One-Hot Encoding : cabin, traveller_type\n",
        "3. Columns with multiple unique categories are encoded using Frequency encoding: route, airline\n",
        "4. 'customer_review' column is dropped as it is not directly useful here.\n",
        "5. Converting 'date_flown' to datetime."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop one feature from pairs with correlation > 0.85(threshold).\n",
        "\n",
        "# Identify highly correlated features\n",
        "corr_matrix = reviews_pre_df.corr(numeric_only=True)\n",
        "high_corr_pairs = set()\n",
        "\n",
        "# Threshold for correlation\n",
        "threshold = 0.85\n",
        "\n",
        "# Identify pairs with high correlation\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
        "            col1 = corr_matrix.columns[i]\n",
        "            col2 = corr_matrix.columns[j]\n",
        "            high_corr_pairs.add((col1, col2))\n",
        "\n",
        "# Display correlated pairs\n",
        "print(\"Highly correlated pairs:\")\n",
        "for pair in high_corr_pairs:\n",
        "    print(pair)\n",
        "\n",
        "# Drop one feature from each pair (e.g., drop second feature)\n",
        "features_to_drop = [pair[1] for pair in high_corr_pairs]\n",
        "\n",
        "# Drop the features\n",
        "reviews_pre_df.drop(features_to_drop, axis=1, inplace=True)\n",
        "print(f\" Dropped highly correlated features: {features_to_drop}\")\n"
      ],
      "metadata": {
        "id": "YEP2MMfRrC_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Manually dropped many irrelevant columns from the table.\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Already performed"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data is transformed to get better resuilts while applying Machine Learning Models.\n",
        "1. Categorical variables are encoded.\n",
        "2. Missing values were handled\n",
        "3. outlier is handled( No outlier was found).\n",
        "4. Datetime conversion is done.\n"
      ],
      "metadata": {
        "id": "UylRnw6CtVo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "qeKqqCLCDx04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Z score scaling\n",
        "\n",
        "#  Dynamically select numerical columns\n",
        "num_cols = reviews_pre_df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "#  Apply Standardization\n",
        "scaler = StandardScaler()\n",
        "reviews_pre_df[num_cols] = scaler.fit_transform(reviews_pre_df[num_cols])\n",
        "\n",
        "#  Display the scaled dataset\n",
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "T23A_PbUv-cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z score scaling to scale the data."
      ],
      "metadata": {
        "id": "Zar6WhvXwXsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because,\n",
        "1. Centers the data around a mean of 0 with a standard deviation of 1, making it ideal for models sensitive to feature magnitude.\n",
        "\n",
        "2. Handles outliers better compared to Min-Max scaling, as it reduces the influence of extreme values.\n",
        "\n",
        "3. Improves model performance by standardizing the distribution, ensuring consistent feature scaling."
      ],
      "metadata": {
        "id": "YZcmeG-UwmdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.shape"
      ],
      "metadata": {
        "id": "YghQqlEoqul1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " No, it is not required for this dataset because:\n",
        "\n",
        "1. Low Feature Count: The dataset has only 16 columns, which is manageable and does not suffer from the curse of dimensionality.\n",
        "\n",
        "2. Feature-to-Row Ratio: With 61,183 rows, the dataset has sufficient samples per feature, making dimensionality reduction unnecessary.\n",
        "\n",
        "3. Risk of Information Loss: Applying dimensionality reduction may remove important information, which could negatively impact model performance"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction\n",
        "# Not required"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not needed"
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the datatype of column 'year_month'"
      ],
      "metadata": {
        "id": "jVawVNwAyKlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "\n",
        "# Splitting the data (80% train, 20% test)\n",
        "X = reviews_pre_df.drop('recommended_encoded', axis=1)  # Features\n",
        "y = reviews_pre_df['recommended_encoded']  # Target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y)\n",
        "\n",
        "# Display the shapes\n",
        "print(f\"Training Set: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test Set: {X_test.shape}, {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratio: 80% for training and 20% for testing. Because,\n",
        "1. Sufficient Training Data:Allocating 80% of the data for training ensures the model learns effectively with more samples, improving its accuracy and stability.\n",
        "\n",
        "2. Reliable Testing Evaluation: The 20% testing set provides enough data to evaluate the model’s performance without overfitting or underfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to check for imbalanced data\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(\"Class Distribution:\")\n",
        "print(y_train.value_counts(normalize=True))  # Percentage distribution\n",
        "print(y_test.value_counts(normalize=True))\n",
        "\n",
        "# Visualizing the class distribution\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.countplot(x=y_train, palette='coolwarm')\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yL4hTcJBy0AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, the dataset is not significantly imbalanced. The class distribution shows 53.43% for the \"Not Recommended\" class and 46.57% for the \"Recommended\" class, resulting in only a 7% difference.\n",
        "\n",
        "Since both classes have fairly similar proportions, the dataset does not require class balancing techniques."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Not required"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is somewhat balanced."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'X_train', 'X_test', 'y_train', and 'y_test' are already created\n",
        "print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "_V5i7viZ26f-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 : Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.head()"
      ],
      "metadata": {
        "id": "7oTtqq8z4XWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_pre_df.info()"
      ],
      "metadata": {
        "id": "2-svgmmEuCMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation - Logistic Regression\n",
        "\n",
        "#  Check the data type of the target variable\n",
        "print(\"y_train data type:\", y_train.dtype)\n",
        "print(\"y_test data type:\", y_test.dtype)\n",
        "\n",
        "#  Convert target variable to discrete classes if it's continuous\n",
        "if y_train.dtype != 'int' and y_train.dtype != 'bool':\n",
        "    y_train = y_train.round().astype(int)\n",
        "    y_test = y_test.round().astype(int)\n",
        "\n",
        "#  Start Timer\n",
        "start = time.time()\n",
        "\n",
        "#  Train Logistic Regression Model\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predictions on Train and Test Sets\n",
        "y_train_pred = lr_model.predict(X_train)  # Train Set Predictions\n",
        "y_test_pred = lr_model.predict(X_test)    # Test Set Predictions\n",
        "\n",
        "#  Train Set Metrics\n",
        "print(\"\\n--- Logistic Regression - Train Set ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_train, y_train_pred):.4f}\")\n",
        "print(\"\\nClassification Report (Train Set):\")\n",
        "print(classification_report(y_train, y_train_pred))\n",
        "\n",
        "#  Test Set Metrics\n",
        "print(\"\\n--- Logistic Regression - Test Set ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_test_pred):.4f}\")\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(classification_report(y_test, y_test_pred))\n",
        "\n",
        "#  End Timer\n",
        "end = time.time()\n",
        "print(f\"\\nTime taken: {round(end - start, 2)} seconds\")\n"
      ],
      "metadata": {
        "id": "wQ74uzmuMMAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing cross validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(lr_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# Display results\n",
        "print(\"\\nCross-Validation Scores:\", cv_scores)\n",
        "print(\"Mean Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation:\", cv_scores.std())\n"
      ],
      "metadata": {
        "id": "lLrZ2VFC5R40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning curve plot to check for overfitting:\n",
        "\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Plot learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    lr_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Mean and std deviation for train and validation scores\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_sizes, train_mean, label='Training Accuracy', color='blue')\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)\n",
        "plt.plot(train_sizes, val_mean, label='Validation Accuracy', color='orange')\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, color='orange', alpha=0.2)\n",
        "\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Learning Curve: Logistic Regression')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b9Fm_8pY5afL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph it is clear that there is no overfitting. Both training accuracy and validation accuracy are high."
      ],
      "metadata": {
        "id": "-HB6c6l35Hlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualizing evaluation Metric Score chart\n",
        "\n",
        "#  Train set metrics\n",
        "train_metrics = [\n",
        "    accuracy_score(y_train, y_train_pred),\n",
        "    precision_score(y_train, y_train_pred),\n",
        "    recall_score(y_train, y_train_pred),\n",
        "    f1_score(y_train, y_train_pred),\n",
        "    roc_auc_score(y_train, y_train_pred)\n",
        "]\n",
        "\n",
        "#  Test set metrics\n",
        "test_metrics = [\n",
        "    accuracy_score(y_test, y_test_pred),\n",
        "    precision_score(y_test, y_test_pred),\n",
        "    recall_score(y_test, y_test_pred),\n",
        "    f1_score(y_test, y_test_pred),\n",
        "    roc_auc_score(y_test, y_test_pred)\n",
        "]\n",
        "\n",
        "#  Metric names\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "\n",
        "#  Plotting\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "x = range(len(metrics))\n",
        "\n",
        "# Bar width\n",
        "width = 0.35\n",
        "\n",
        "#  Plotting train and test metrics\n",
        "ax.bar([p - width/2 for p in x], train_metrics, width, label='Train Set', color='cornflowerblue')\n",
        "ax.bar([p + width/2 for p in x], test_metrics, width, label='Test Set', color='lightcoral')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Logistic Regression - Model Performance (Train vs Test)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Display score values above the bars\n",
        "for i in range(len(metrics)):\n",
        "    ax.text(i - width/2, train_metrics[i] + 0.01, f\"{train_metrics[i]:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "    ax.text(i + width/2, test_metrics[i] + 0.01, f\"{test_metrics[i]:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4SftwMMTQS_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        " # Hyperparameter tuning is not required in this case.The model already demonstrates high and stable performance, making additional tuning unnecessary."
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning is not essential in this case.\n",
        "\n",
        "* The model already demonstrates high and stable performance, making additional tuning unnecessary.\n",
        "\n",
        "* Cross-validation could still be applied to validate the model’s stability, but it is not mandatory for further improvement.\n",
        "\n",
        "* Tuning may not significantly enhance the model’s performance and could introduce unnecessary complexity or increase runtime.\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tuning is not used here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 : Decision Tree"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 2 : implementation - Decision Tree\n",
        "\n",
        "#  Start Timer\n",
        "start = time.time()\n",
        "\n",
        "#  Initialize Decision Tree Model with default parameters\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "#  Train the model\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predictions on Train and Test Sets\n",
        "y_train_pred_dt = dt_model.predict(X_train)  # Train Set Predictions\n",
        "y_test_pred_dt = dt_model.predict(X_test)    # Test Set Predictions\n",
        "\n",
        "#  Train Set Metrics\n",
        "train_metrics = {\n",
        "    \"Dataset\": \"Train Set\",\n",
        "    \"Accuracy\": accuracy_score(y_train, y_train_pred_dt),\n",
        "    \"Precision\": precision_score(y_train, y_train_pred_dt),\n",
        "    \"Recall\": recall_score(y_train, y_train_pred_dt),\n",
        "    \"F1-Score\": f1_score(y_train, y_train_pred_dt),\n",
        "    \"ROC-AUC\": roc_auc_score(y_train, y_train_pred_dt)\n",
        "}\n",
        "\n",
        "#  Test Set Metrics\n",
        "test_metrics = {\n",
        "    \"Dataset\": \"Test Set\",\n",
        "    \"Accuracy\": accuracy_score(y_test, y_test_pred_dt),\n",
        "    \"Precision\": precision_score(y_test, y_test_pred_dt),\n",
        "    \"Recall\": recall_score(y_test, y_test_pred_dt),\n",
        "    \"F1-Score\": f1_score(y_test, y_test_pred_dt),\n",
        "    \"ROC-AUC\": roc_auc_score(y_test, y_test_pred_dt)\n",
        "}\n",
        "\n",
        "#  Create DataFrame for Comparison\n",
        "metrics_df = pd.DataFrame([train_metrics, test_metrics])\n",
        "\n",
        "#  Display Metrics\n",
        "print(\"\\n---  Decision Tree Model - Evaluation Metrics ---\")\n",
        "print(metrics_df)\n",
        "\n",
        "#  Print Classification Reports\n",
        "print(\"\\n---  Classification Report (Train Set) ---\")\n",
        "print(classification_report(y_train, y_train_pred_dt))\n",
        "\n",
        "print(\"\\n---  Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_dt))\n",
        "\n",
        "#  End Timer\n",
        "end = time.time()\n",
        "print(f\"\\n⏱ Time taken: {round(end - start, 2)} seconds\")\n"
      ],
      "metadata": {
        "id": "34a_npzRSawV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "#  Extracting the metrics for visualization\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "train_scores = [\n",
        "    accuracy_score(y_train, y_train_pred_dt),\n",
        "    precision_score(y_train, y_train_pred_dt),\n",
        "    recall_score(y_train, y_train_pred_dt),\n",
        "    f1_score(y_train, y_train_pred_dt),\n",
        "    roc_auc_score(y_train, y_train_pred_dt)\n",
        "]\n",
        "\n",
        "test_scores = [\n",
        "    accuracy_score(y_test, y_test_pred_dt),\n",
        "    precision_score(y_test, y_test_pred_dt),\n",
        "    recall_score(y_test, y_test_pred_dt),\n",
        "    f1_score(y_test, y_test_pred_dt),\n",
        "    roc_auc_score(y_test, y_test_pred_dt)\n",
        "]\n",
        "\n",
        "#  Plotting the scores\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "#  Bars for Train and Test Scores\n",
        "bars1 = ax.bar(x - width/2, train_scores, width, label='Train Set', color='skyblue')\n",
        "bars2 = ax.bar(x + width/2, test_scores, width, label='Test Set', color='lightcoral')\n",
        "\n",
        "#  Adding value labels above bars\n",
        "for bar, score in zip(bars1, train_scores):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "            f\"{score:.4f}\", ha='center', fontsize=10, color='black')\n",
        "\n",
        "for bar, score in zip(bars2, test_scores):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "            f\"{score:.4f}\", ha='center', fontsize=10, color='black')\n",
        "\n",
        "#  Labels and Title\n",
        "ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Decision Tree Model - Train vs Test Set Performance', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Grid and formatting\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0eveEK5eTFAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "\n",
        "# Define the parameter grid for Decision Tree\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, 20, None],        # Control depth of the tree\n",
        "    'min_samples_split': [2, 5, 10, 15],        # Minimum samples to split a node\n",
        "    'min_samples_leaf': [1, 5, 10],             # Minimum samples per leaf\n",
        "    'criterion': ['gini', 'entropy'],           # Split criteria\n",
        "    'random_state': [42]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply gridsearchCV\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt_model,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,                      # 5-fold cross-validation\n",
        "                           scoring='accuracy',        # Use accuracy as the metric\n",
        "                           n_jobs=-1,                 # Use all available CPU cores\n",
        "                           verbose=2)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display the best parameters and best score\n",
        "print(\"\\nBest Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "YoXvXO7IJlpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model\n",
        "\n",
        "# Get the best model from GridSearchCV\n",
        "best_dt_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt_tuned = best_dt_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_dt_tuned)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_dt_tuned)\n",
        "precision = precision_score(y_test, y_pred_dt_tuned)\n",
        "recall = recall_score(y_test, y_pred_dt_tuned)\n",
        "f1 = f1_score(y_test, y_pred_dt_tuned)\n",
        "\n",
        "# Display metrics\n",
        "print(\"\\n--- Tuned Decision Tree ---\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "WYvI5bt6JxIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridsearchCV is used here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is significance improvement is found in evaluation metrics after using GridsearchCV."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visaulization - Decision Tree - Model Performance Before vs After Tuning\n",
        "\n",
        "#  Metrics Before Tuning - Decision Tree\n",
        "accuracy_before_dt = 0.9067\n",
        "roc_auc_before_dt = 0.9063\n",
        "precision_before_dt = 0.8987\n",
        "recall_before_dt = 0.9012\n",
        "f1_before_dt = 0.8999\n",
        "\n",
        "#  Metrics After Tuning - Decision Tree (use actual values)\n",
        "accuracy_after_dt = accuracy_score(y_test, y_pred_dt_tuned)\n",
        "roc_auc_after_dt = roc_auc_score(y_test, y_pred_dt_tuned)\n",
        "precision_after_dt = precision_score(y_test, y_pred_dt_tuned)\n",
        "recall_after_dt = recall_score(y_test, y_pred_dt_tuned)\n",
        "f1_after_dt = f1_score(y_test, y_pred_dt_tuned)\n",
        "\n",
        "# Labels\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "# Prepare Data for Plotting\n",
        "before_dt = [accuracy_before_dt, roc_auc_before_dt, precision_before_dt, recall_before_dt, f1_before_dt]\n",
        "after_dt = [accuracy_after_dt, roc_auc_after_dt, precision_after_dt, recall_after_dt, f1_after_dt]\n",
        "\n",
        "# Plotting\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "ax.bar(x - width/2, before_dt, width, label='Before Tuning', color='lightcoral')\n",
        "ax.bar(x + width/2, after_dt, width, label='After Tuning', color='seagreen')\n",
        "\n",
        "# Labels and Formatting\n",
        "ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Decision Tree - Model Performance Before vs After Tuning', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "# Display values on bars\n",
        "for i in range(len(metrics)):\n",
        "    ax.text(i - width/2, before_dt[i] + 0.005, f\"{before_dt[i]:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "    ax.text(i + width/2, after_dt[i] + 0.005, f\"{after_dt[i]:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pfM7JMrjScRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Accuracy (93.55%):** The model correctly predicts 93.55% of the cases, indicating overall good performance. However, accuracy alone may not reflect class imbalances.\n",
        "\n",
        "**2. ROC-AUC Score (93.42%):** The model effectively distinguishes between satisfied and dissatisfied customers, making it reliable for customer classification tasks.\n",
        "\n",
        "**3. Precision (94.51%):** 94.51% of the customers predicted as satisfied are actually satisfied, reducing the risk of false positives, which helps in targeting the right customer group.\n",
        "\n",
        "**4. Recall (91.47%):** The model correctly identifies 91.47% of actual satisfied customers, ensuring fewer dissatisfied customers are misclassified, which enhances customer retention efforts.\n",
        "\n",
        "**5. F1-Score (92.96%):**  Balances precision and recall, indicating the model maintains a strong trade-off, making it effective for business decisions related to customer satisfaction prediction."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 : Random Forest\n",
        "\n"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ML Model - 3 Implementation - Random Forest\n",
        "\n",
        "#  Start time\n",
        "start = time.time()\n",
        "\n",
        "#  Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,         # Number of trees\n",
        "    max_depth=10,             # Limit depth to prevent overfitting\n",
        "    min_samples_split=10,     # Minimum samples required to split a node\n",
        "    min_samples_leaf=5,       # Minimum samples required at each leaf node\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "#  Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions on both Train and Test Sets\n",
        "y_train_pred_rf = rf_model.predict(X_train)   # Train set predictions\n",
        "y_test_pred_rf = rf_model.predict(X_test)     # Test set predictions\n",
        "\n",
        "#  Calculate metrics\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "#  Train Set Metrics\n",
        "train_metrics = [\n",
        "    accuracy_score(y_train, y_train_pred_rf),\n",
        "    roc_auc_score(y_train, y_train_pred_rf),\n",
        "    precision_score(y_train, y_train_pred_rf),\n",
        "    recall_score(y_train, y_train_pred_rf),\n",
        "    f1_score(y_train, y_train_pred_rf)\n",
        "]\n",
        "\n",
        "#  Test Set Metrics\n",
        "test_metrics = [\n",
        "    accuracy_score(y_test, y_test_pred_rf),\n",
        "    roc_auc_score(y_test, y_test_pred_rf),\n",
        "    precision_score(y_test, y_test_pred_rf),\n",
        "    recall_score(y_test, y_test_pred_rf),\n",
        "    f1_score(y_test, y_test_pred_rf)\n",
        "]\n",
        "\n",
        "#  Create DataFrame for easy comparison\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Metric': metrics,\n",
        "    'Train Set': train_metrics,\n",
        "    'Test Set': test_metrics\n",
        "})\n",
        "\n",
        "#  Format the scores to 4 decimal places\n",
        "metrics_df = metrics_df.round(4)\n",
        "\n",
        "#  Display the table\n",
        "print(\"\\n---  Random Forest - Evaluation Metrics ---\")\n",
        "print(metrics_df)\n",
        "\n",
        "#  End time\n",
        "end = time.time()\n",
        "print(f\"\\n⏱️ Time taken: {round(end - start, 2)} seconds\")\n"
      ],
      "metadata": {
        "id": "ZNYwKwzJWF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualizing evaluation Metric Score chart\n",
        "\n",
        "#  Labels and Scores\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "train_scores = [\n",
        "    accuracy_score(y_train, y_train_pred_rf),\n",
        "    roc_auc_score(y_train, y_train_pred_rf),\n",
        "    precision_score(y_train, y_train_pred_rf),\n",
        "    recall_score(y_train, y_train_pred_rf),\n",
        "    f1_score(y_train, y_train_pred_rf)\n",
        "]\n",
        "\n",
        "test_scores = [\n",
        "    accuracy_score(y_test, y_test_pred_rf),\n",
        "    roc_auc_score(y_test, y_test_pred_rf),\n",
        "    precision_score(y_test, y_test_pred_rf),\n",
        "    recall_score(y_test, y_test_pred_rf),\n",
        "    f1_score(y_test, y_test_pred_rf)\n",
        "]\n",
        "\n",
        "#  Plotting the chart\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "x = range(len(metrics))\n",
        "bar_width = 0.35\n",
        "\n",
        "# Plot Train and Test Scores\n",
        "bars1 = ax.bar([p - bar_width/2 for p in x], train_scores, bar_width, label='Train Set', color='#4CAF50')\n",
        "bars2 = ax.bar([p + bar_width/2 for p in x], test_scores, bar_width, label='Test Set', color='#2196F3')\n",
        "\n",
        "#  Add score values on top of the bars\n",
        "for bar, score in zip(bars1, train_scores):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, f\"{score:.4f}\", ha='center', fontsize=10, color='black')\n",
        "\n",
        "for bar, score in zip(bars2, test_scores):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, f\"{score:.4f}\", ha='center', fontsize=10, color='black')\n",
        "\n",
        "#  Labels and formatting\n",
        "ax.set_title('Random Forest - Train vs Test Set Evaluation Metrics', fontsize=16, fontweight='bold')\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Styling\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "\n",
        "#  Display the chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-HViO68hX28R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Not required"
      ],
      "metadata": {
        "id": "_Exkh3JND08P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not required.\n",
        "* The Train and Test set metrics are consistently high, showing no signs of overfitting or underfitting.\n",
        "\n",
        "* The model has balanced precision, recall, and F1-scores, indicating it handles both classes effectively."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ML Model 4: XGBoost"
      ],
      "metadata": {
        "id": "O2GOxdR0B8p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Map target labels to 0 and 1\n",
        "y_train = y_train.replace({-1: 0, 1: 1})\n",
        "y_test = y_test.replace({-1: 0, 1: 1})\n",
        "\n",
        "#  Verify the mapping\n",
        "print(\"Unique values in y_train:\", y_train.unique())\n",
        "print(\"Unique values in y_test:\", y_test.unique())\n"
      ],
      "metadata": {
        "id": "qO3DR5ffVaiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 4 -  Implementation - XGBoost\n",
        "\n",
        "#  Start time\n",
        "start = time.time()\n",
        "\n",
        "# Create an instance of the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "\n",
        "#  Train the model\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predictions on Train and Test sets\n",
        "y_train_pred_xgb = xgb_model.predict(X_train)\n",
        "y_test_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "#  Train set metrics\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred_xgb)\n",
        "train_roc_auc = roc_auc_score(y_train, y_train_pred_xgb)\n",
        "train_precision = precision_score(y_train, y_train_pred_xgb)\n",
        "train_recall = recall_score(y_train, y_train_pred_xgb)\n",
        "train_f1 = f1_score(y_train, y_train_pred_xgb)\n",
        "\n",
        "#  Test set metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_xgb)\n",
        "test_roc_auc = roc_auc_score(y_test, y_test_pred_xgb)\n",
        "test_precision = precision_score(y_test, y_test_pred_xgb)\n",
        "test_recall = recall_score(y_test, y_test_pred_xgb)\n",
        "test_f1 = f1_score(y_test, y_test_pred_xgb)\n",
        "\n",
        "#  Create a comparison table\n",
        "\n",
        "\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Train Set': [train_accuracy, train_roc_auc, train_precision, train_recall, train_f1],\n",
        "    'Test Set': [test_accuracy, test_roc_auc, test_precision, test_recall, test_f1]\n",
        "})\n",
        "\n",
        "#  Display the table\n",
        "print(\"\\n---  XGBoost - Evaluation Metrics ---\")\n",
        "print(metrics_table)\n",
        "\n",
        "#  Confusion matrices\n",
        "print(\"\\n--- Confusion Matrix (Train Set) ---\")\n",
        "print(confusion_matrix(y_train, y_train_pred_xgb))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
        "print(confusion_matrix(y_test, y_test_pred_xgb))\n",
        "\n",
        "#  Classification reports\n",
        "print(\"\\n--- Classification Report (Train Set) ---\")\n",
        "print(classification_report(y_train, y_train_pred_xgb))\n",
        "\n",
        "print(\"\\n--- Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_xgb))\n",
        "\n",
        "#  End time\n",
        "end = time.time()\n",
        "print(f\"\\n Time taken: {round(end - start, 2)} seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nQ8-B05uZVss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWu1f3k9CIC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "#  Metrics for Train and Test sets\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "train_scores = [train_accuracy, train_roc_auc, train_precision, train_recall, train_f1]\n",
        "test_scores = [test_accuracy, test_roc_auc, test_precision, test_recall, test_f1]\n",
        "\n",
        "#  Plotting the comparison chart\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Bars for Train and Test sets\n",
        "bars1 = ax.bar(x - width/2, train_scores, width, label='Train Set', color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, test_scores, width, label='Test Set', color='orange')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('XGBoost Model - Evaluation Metrics (Train vs Test)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Display score values on bars\n",
        "for bars, scores in zip([bars1, bars2], [train_scores, test_scores]):\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
        "                f\"{score:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "#  Grid and layout adjustments\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#  Plotting Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Train Set Confusion Matrix\n",
        "sns.heatmap(confusion_matrix(y_train, y_train_pred_xgb), annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "axes[0].set_title('Train Set Confusion Matrix')\n",
        "axes[0].set_xlabel('Predicted Labels')\n",
        "axes[0].set_ylabel('True Labels')\n",
        "\n",
        "# Test Set Confusion Matrix\n",
        "sns.heatmap(confusion_matrix(y_test, y_test_pred_xgb), annot=True, fmt='d', cmap='Oranges', ax=axes[1])\n",
        "axes[1].set_title('Test Set Confusion Matrix')\n",
        "axes[1].set_xlabel('Predicted Labels')\n",
        "axes[1].set_ylabel('True Labels')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OVd6tOOxe2HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "gMDnnEgoCYw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # ML Model - 4 Implementation with hyperparameter optimization techniques- randomizedsearchCV\n",
        "\n",
        "#  Start Timer\n",
        "start = time.time()\n",
        "\n",
        "#  XGBoost Model Initialization\n",
        "xgb_model = XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "#  Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'max_depth': [3, 5, 7, 10],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'min_child_weight': [1, 3, 5, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3],\n",
        "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
        "    'reg_lambda': [0, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "#  RandomizedSearchCV with 5-fold Cross-Validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=50,              # Number of random parameter combinations\n",
        "    scoring='accuracy',     # Evaluation metric\n",
        "    cv=5,                   # 5-fold cross-validation\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "#  Fit the model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "#  Best model and parameters\n",
        "best_xgb_model = random_search.best_estimator_\n",
        "print(\"\\n Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "#  Predictions on Train and Test Sets\n",
        "y_train_pred_xgb_tuned = best_xgb_model.predict(X_train)\n",
        "y_test_pred_xgb_tuned = best_xgb_model.predict(X_test)\n",
        "\n",
        "#  Train Set Metrics\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred_xgb_tuned)\n",
        "train_roc_auc = roc_auc_score(y_train, y_train_pred_xgb_tuned)\n",
        "train_precision = precision_score(y_train, y_train_pred_xgb_tuned)\n",
        "train_recall = recall_score(y_train, y_train_pred_xgb_tuned)\n",
        "train_f1 = f1_score(y_train, y_train_pred_xgb_tuned)\n",
        "\n",
        "#  Test Set Metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_xgb_tuned)\n",
        "test_roc_auc = roc_auc_score(y_test, y_test_pred_xgb_tuned)\n",
        "test_precision = precision_score(y_test, y_test_pred_xgb_tuned)\n",
        "test_recall = recall_score(y_test, y_test_pred_xgb_tuned)\n",
        "test_f1 = f1_score(y_test, y_test_pred_xgb_tuned)\n",
        "\n",
        "#  Create a comparison table\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Train Set': [train_accuracy, train_roc_auc, train_precision, train_recall, train_f1],\n",
        "    'Test Set': [test_accuracy, test_roc_auc, test_precision, test_recall, test_f1]\n",
        "})\n",
        "\n",
        "#  Display the table\n",
        "print(\"\\n---  XGBoost Model - Evaluation Metrics (After Tuning) ---\")\n",
        "print(metrics_table)\n",
        "\n",
        "#  Confusion matrices\n",
        "print(\"\\n--- Confusion Matrix (Train Set) ---\")\n",
        "print(confusion_matrix(y_train, y_train_pred_xgb_tuned))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
        "print(confusion_matrix(y_test, y_test_pred_xgb_tuned))\n",
        "\n",
        "#  Classification reports\n",
        "print(\"\\n--- Classification Report (Train Set) ---\")\n",
        "print(classification_report(y_train, y_train_pred_xgb_tuned))\n",
        "\n",
        "print(\"\\n--- Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_xgb_tuned))\n",
        "\n",
        "#  End Timer\n",
        "end = time.time()\n",
        "print(f\"\\n Time taken: {round(end - start, 2)} seconds\")\n"
      ],
      "metadata": {
        "id": "lWKXPj21iEdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "wVSDxV_gCzHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV because it efficiently explores a wide range of hyperparameter combinations in less time compared to GridSearchCV, making it suitable for faster tuning with large datasets."
      ],
      "metadata": {
        "id": "rgrE7JJfC0lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "FrUJscTWC4Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Visualization of comparison between before and after tuning of evalation metrics values.\n",
        "\n",
        "#  Metrics and Scores\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "#  Before Tuning Scores (replace these with actual before-tuning scores)\n",
        "before_tuning = [ 0.941162, 0.940760, 0.938524, 0.934901, 0.936709]\n",
        "\n",
        "#  After Tuning Scores\n",
        "after_tuning = [\n",
        "    test_accuracy,\n",
        "    test_roc_auc,\n",
        "    test_precision,\n",
        "    test_recall,\n",
        "    test_f1\n",
        "]\n",
        "\n",
        "#  Plotting\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Bars for Before and After Tuning\n",
        "bars1 = ax.bar(x - width/2, before_tuning, width, label='Before Tuning', color='lightcoral')\n",
        "bars2 = ax.bar(x + width/2, after_tuning, width, label='After Tuning', color='seagreen')\n",
        "\n",
        "#  Labels and Formatting\n",
        "ax.set_xlabel('Evaluation Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('XGBoost Model - Comparison of Evaluation Metrics (Before vs After Tuning)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Display score values on bars\n",
        "for bars, scores in zip([bars1, bars2], [before_tuning, after_tuning]):\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
        "                f\"{score:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "#  Grid and Layout\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7QmOMigDdFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is small amount of change in evaluation metrics ofger tuning."
      ],
      "metadata": {
        "id": "WCatsfw4m5nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model 5: LightGBM"
      ],
      "metadata": {
        "id": "uz7W5xAjC7kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model -5  Implementation - LightGBM\n",
        "\n",
        "#  Start Timer\n",
        "start = time.time()\n",
        "\n",
        "#  Initialize LightGBM model with basic parameters\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    boosting_type='gbdt',\n",
        "    objective='binary',        # For binary classification\n",
        "    n_estimators=100,          # Number of boosting rounds\n",
        "    learning_rate=0.1,         # Step size shrinkage\n",
        "    max_depth=10,              # Maximum tree depth\n",
        "    num_leaves=31,             # Number of leaves in each tree\n",
        "    min_child_samples=20,      # Minimum samples in leaf\n",
        "    subsample=0.8,             # Subsample ratio for rows\n",
        "    colsample_bytree=0.8,      # Subsample ratio for columns\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "#  Fit the model\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "#  Make predictions\n",
        "y_train_pred_lgb = lgb_model.predict(X_train)\n",
        "y_test_pred_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "#  Train Set Metrics\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred_lgb)\n",
        "train_roc_auc = roc_auc_score(y_train, y_train_pred_lgb)\n",
        "train_precision = precision_score(y_train, y_train_pred_lgb)\n",
        "train_recall = recall_score(y_train, y_train_pred_lgb)\n",
        "train_f1 = f1_score(y_train, y_train_pred_lgb)\n",
        "\n",
        "#  Test Set Metrics\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred_lgb)\n",
        "test_roc_auc = roc_auc_score(y_test, y_test_pred_lgb)\n",
        "test_precision = precision_score(y_test, y_test_pred_lgb)\n",
        "test_recall = recall_score(y_test, y_test_pred_lgb)\n",
        "test_f1 = f1_score(y_test, y_test_pred_lgb)\n",
        "\n",
        "#  Create a comparison table\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Train Set': [train_accuracy, train_roc_auc, train_precision, train_recall, train_f1],\n",
        "    'Test Set': [test_accuracy, test_roc_auc, test_precision, test_recall, test_f1]\n",
        "})\n",
        "\n",
        "#  Display the table\n",
        "print(\"\\n---  LightGBM - Evaluation Metrics ---\")\n",
        "print(metrics_table)\n",
        "\n",
        "#  Confusion Matrices\n",
        "print(\"\\n--- Confusion Matrix (Train Set) ---\")\n",
        "print(confusion_matrix(y_train, y_train_pred_lgb))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
        "print(confusion_matrix(y_test, y_test_pred_lgb))\n",
        "\n",
        "# Classification Reports\n",
        "print(\"\\n--- Classification Report (Train Set) ---\")\n",
        "print(classification_report(y_train, y_train_pred_lgb))\n",
        "\n",
        "print(\"\\n--- Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_lgb))\n",
        "\n",
        "#  End Timer\n",
        "end = time.time()\n",
        "print(f\"\\n Time taken: {round(end - start, 2)} seconds\")\n"
      ],
      "metadata": {
        "id": "-sNMQMcynX1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ToRaHYB8DEUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "\n",
        "#  Metrics and scores\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "train_scores = [train_accuracy, train_roc_auc, train_precision, train_recall, train_f1]\n",
        "test_scores = [test_accuracy, test_roc_auc, test_precision, test_recall, test_f1]\n",
        "\n",
        "#  Plotting the comparison chart\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Bars for Train and Test sets\n",
        "bars1 = ax.bar(x - width/2, train_scores, width, label='Train Set', color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, test_scores, width, label='Test Set', color='orange')\n",
        "\n",
        "#  Labels and formatting\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('LightGBM Model - Evaluation Metrics (Train vs Test)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Display score values on bars\n",
        "for bars, scores in zip([bars1, bars2], [train_scores, test_scores]):\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
        "                f\"{score:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "#  Grid and layout adjustments\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sCVvQ_lQpjwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "2J68WYtoDQ9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross validation and Hyperparameter tuning is not requred here. But to get accurate results,  Stratified K-Fold Cross-Validation and RandomizedSearchCV is used."
      ],
      "metadata": {
        "id": "suUR_KsxEBx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#   Code for Stratified K-Fold Cross-Validation and RandomizedSearchCV\n",
        "\n",
        "#  Define the LightGBM model\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42)\n",
        "\n",
        "#  Hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300, 400],          # Number of boosting rounds\n",
        "    'max_depth': [-1, 10, 15, 20],                 # Max tree depth (-1 means no limit)\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],       # Step size\n",
        "    'num_leaves': [31, 50, 70, 100],               # Number of leaves in each tree\n",
        "    'min_child_samples': [10, 20, 30, 50],         # Min samples in a leaf\n",
        "    'subsample': [0.6, 0.8, 1.0],                   # Row sampling\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],            # Feature sampling\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],                 # L1 regularization\n",
        "    'reg_lambda': [0, 0.1, 0.5, 1]                 # L2 regularization\n",
        "}\n",
        "\n",
        "#  Stratified K-Fold Cross-Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "#  RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=lgb_model,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=25,                        # 25 random combinations\n",
        "    scoring='accuracy',               # Evaluation metric\n",
        "    cv=cv,                            # 5-fold cross-validation\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1                         # Use all CPUs\n",
        ")\n",
        "\n",
        "#  Start timer\n",
        "start = time.time()\n",
        "\n",
        "#  Fit the model using RandomizedSearchCV\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "#  Best hyperparameters\n",
        "print(\"\\n Best Hyperparameters:\", random_search.best_params_)\n",
        "\n",
        "#  Train the LightGBM model with the best parameters\n",
        "best_lgb_model = random_search.best_estimator_\n",
        "\n",
        "#  Fit the optimized model\n",
        "best_lgb_model.fit(X_train, y_train)\n",
        "\n",
        "#  Predictions on Train and Test sets\n",
        "y_train_pred_lgb_tuned = best_lgb_model.predict(X_train)\n",
        "y_test_pred_lgb_tuned = best_lgb_model.predict(X_test)\n",
        "\n",
        "#  Train Set Metrics\n",
        "train_accuracy_tuned = accuracy_score(y_train, y_train_pred_lgb_tuned)\n",
        "train_roc_auc_tuned = roc_auc_score(y_train, y_train_pred_lgb_tuned)\n",
        "train_precision_tuned = precision_score(y_train, y_train_pred_lgb_tuned)\n",
        "train_recall_tuned = recall_score(y_train, y_train_pred_lgb_tuned)\n",
        "train_f1_tuned = f1_score(y_train, y_train_pred_lgb_tuned)\n",
        "\n",
        "# Test Set Metrics\n",
        "test_accuracy_tuned = accuracy_score(y_test, y_test_pred_lgb_tuned)\n",
        "test_roc_auc_tuned = roc_auc_score(y_test, y_test_pred_lgb_tuned)\n",
        "test_precision_tuned = precision_score(y_test, y_test_pred_lgb_tuned)\n",
        "test_recall_tuned = recall_score(y_test, y_test_pred_lgb_tuned)\n",
        "test_f1_tuned = f1_score(y_test, y_test_pred_lgb_tuned)\n",
        "\n",
        "# Create a comparison table\n",
        "metrics_table_tuned = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
        "    'Train Set': [train_accuracy_tuned, train_roc_auc_tuned, train_precision_tuned, train_recall_tuned, train_f1_tuned],\n",
        "    'Test Set': [test_accuracy_tuned, test_roc_auc_tuned, test_precision_tuned, test_recall_tuned, test_f1_tuned]\n",
        "})\n",
        "\n",
        "#  Display the table\n",
        "print(\"\\n---  LightGBM Model (After Tuning) - Evaluation Metrics ---\")\n",
        "print(metrics_table_tuned)\n",
        "\n",
        "# Confusion Matrices\n",
        "print(\"\\n--- Confusion Matrix (Train Set) ---\")\n",
        "print(confusion_matrix(y_train, y_train_pred_lgb_tuned))\n",
        "\n",
        "print(\"\\n--- Confusion Matrix (Test Set) ---\")\n",
        "print(confusion_matrix(y_test, y_test_pred_lgb_tuned))\n",
        "\n",
        "# Classification Reports\n",
        "print(\"\\n--- Classification Report (Train Set) ---\")\n",
        "print(classification_report(y_train, y_train_pred_lgb_tuned))\n",
        "\n",
        "print(\"\\n--- Classification Report (Test Set) ---\")\n",
        "print(classification_report(y_test, y_test_pred_lgb_tuned))\n",
        "\n",
        "# End Timer\n",
        "end = time.time()\n",
        "print(f\"\\n Time taken: {round(end - start, 2)} seconds\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTUIZvfhsym0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "iczSF0kHDYzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RandomizedsearchCV is used here."
      ],
      "metadata": {
        "id": "MwviDh0HDdMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "8TusP4ijDf2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of evaluation metrics after tuning\n",
        "\n",
        "#  Metrics and scores\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "train_scores = [train_accuracy_tuned, train_roc_auc_tuned, train_precision_tuned, train_recall_tuned, train_f1_tuned]\n",
        "test_scores = [test_accuracy_tuned, test_roc_auc_tuned, test_precision_tuned, test_recall_tuned, test_f1_tuned]\n",
        "\n",
        "#  Plotting the comparison chart\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Bars for Train and Test sets\n",
        "bars1 = ax.bar(x - width/2, train_scores, width, label='Train Set', color='steelblue')\n",
        "bars2 = ax.bar(x + width/2, test_scores, width, label='Test Set', color='orange')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title(' LightGBM Model - Evaluation Metrics (After Tuning)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Display score values on bars\n",
        "for bars, scores in zip([bars1, bars2], [train_scores, test_scores]):\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
        "                f\"{score:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "#  Grid and layout adjustments\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z5f9mqvuD44w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#   LightGBM - Evaluation Metrics (Before vs After Tuning)\n",
        "\n",
        "# Metrics and scores\n",
        "metrics = ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score']\n",
        "\n",
        "# Scores before tuning (use your actual pre-tuning values)\n",
        "test_scores_before = [0.943695, 0.943334, 0.940866, 0.940866, 0.939461]\n",
        "\n",
        "# Scores after tuning\n",
        "test_scores_after = [0.944594, 0.944197, 0.942379, 0.938410, 0.940390]\n",
        "\n",
        "#  Plotting the comparison chart\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Bars for Before and After Tuning\n",
        "bars1 = ax.bar(x - width/2, test_scores_before, width, label='Before Tuning', color='lightcoral')\n",
        "bars2 = ax.bar(x + width/2, test_scores_after, width, label='After Tuning', color='seagreen')\n",
        "\n",
        "# Labels and formatting\n",
        "ax.set_xlabel('Metrics', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title(' LightGBM - Evaluation Metrics (Before vs After Tuning)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "\n",
        "#  Display score values on bars\n",
        "for bars, scores in zip([bars1, bars2], [test_scores_before, test_scores_after]):\n",
        "    for bar, score in zip(bars, scores):\n",
        "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.005,\n",
        "                f\"{score:.4f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
        "\n",
        "# Grid and layout adjustments\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3mOBZ7PeDxY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no significant difference is found in evaluation metric values."
      ],
      "metadata": {
        "id": "FQU3pGviDjr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying all evaluation metric values from the outputs of all models."
      ],
      "metadata": {
        "id": "zlYsnK4UGeop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create a comparison table with evaluation metrics for all models\n",
        "metrics_comparison = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'ROC-AUC', 'Precision', 'Recall', 'F1-Score'],\n",
        "\n",
        "    # Train Set Metrics\n",
        "    'Logistic Regression (Train)': [ 0.9379, 0.9377, 0.9325, 0.9343, 0.9334],\n",
        "    'Decision Tree (Train)': [0.997896, 0.997822, 0.998725, 0.996754, 0.997738],\n",
        "    'Random Forest (Train)': [0.9476, 0.9470, 0.9480 , 0.9389, 0.9434],\n",
        "    'XGBoost (Train)': [0.954644, 0.954367, 0.952176, 0.950338, 0.951256],\n",
        "    'LightGBM (Train)': [ 0.947697, 0.947363, 0.945055, 0.942485, 0.943768],\n",
        "\n",
        "    # Test Set Metrics\n",
        "    'Logistic Regression (Test)': [0.9381, 0.9377, 0.9338, 0.9331, 0.9335],\n",
        "    'Decision Tree (Test)': [0.9355, 0.9342, 0.9451, 0.9147, 0.9296],\n",
        "    'Random Forest (Test)': [0.9427, 0.9419, 0.9455, 0.9307, 0.9380],\n",
        "    'XGBoost (Test)': [0.942796, 0.942470, 0.939357, 0.937708, 0.938532],\n",
        "    'LightGBM (Test)': [0.944594, 0.944197, 0.942379, 0.938410, 0.940390]\n",
        "})\n",
        "\n",
        "#  Display the comparison table\n",
        "print(\"\\n--- Evaluation Metrics Comparison Across All Models ---\")\n",
        "metrics_comparison\n"
      ],
      "metadata": {
        "id": "FWP0DoujG3aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered Accuracy, ROC-AUC, Precision, Recall, and F1-Score for a positive business impact. Accuracy ensures overall correctness, ROC-AUC measures the model’s ability to distinguish between classes, and F1-Score balances precision and recall, which is crucial for minimizing false predictions and ensuring reliable results."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose LightGBM as the final prediction model because it achieved the highest test accuracy (94.46%), ROC-AUC (94.42%), and F1-score (94.04%), indicating superior performance and generalization. It effectively balances accuracy and robustness, making it the most reliable model."
      ],
      "metadata": {
        "id": "tMRrSKk6O6Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. **Comprehensive Data Preprocessing:** The project began with thorough data cleaning, handling missing values, and performing exploratory data analysis (EDA) to identify patterns, correlations, and outliers.  \n",
        "2. **Feature Engineering and Selection:** Effective feature selection improved model accuracy by retaining only relevant variables, reducing noise, and preventing overfitting.  \n",
        "3. **Model Variety and Comparison:** Multiple machine learning models were implemented, including **Logistic Regression, Decision Tree, Random Forest, XGBoost, and LightGBM**, ensuring a diverse evaluation.  \n",
        "4. **Cross-Validation and Tuning:** Hyperparameter tuning and cross-validation significantly enhanced model performance by preventing overfitting and improving generalization.  \n",
        "5. **LightGBM emerged as the best model**, delivering the highest accuracy (**94.46%**) and F1-Score, making it the final choice for prediction.  \n",
        "6. **XGBoost showed competitive performance**, with slightly lower accuracy than LightGBM but still highly reliable, making it a close alternative.  \n",
        "7. **Decision Tree and Random Forest models overfitted**, showing high training accuracy but lower test accuracy, indicating poor generalization.  \n",
        "8. **Evaluation Metrics Consistency:** Precision, recall, and F1-scores remained consistent for the best-performing models, ensuring reliability across various performance dimensions.   \n",
        "9. **Scalability and Efficiency:** LightGBM was chosen as the final model due to its superior accuracy, faster execution, and scalability, making it suitable for large datasets and real-world deployment."
      ],
      "metadata": {
        "id": "8tsCj7eTQhXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Thank You***"
      ],
      "metadata": {
        "id": "gT3NCzMN14if"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***You have successfully completed your Machine Learning Capstone Project !***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}